{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "deep-dream-sheep.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2aSq4gkxq_z",
        "colab_type": "text"
      },
      "source": [
        "# Interpretability I: Feature Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq0OP7UIxq_1",
        "colab_type": "text"
      },
      "source": [
        "## Copyright notice\n",
        "\n",
        "Parts of this code are adapted from https://pastebin.com/ETXc7Xma and the [Keras example](https://github.com/keras-team/keras/blob/master/examples/conv_filter_visualization.py), (c) 2015 - 2018, Fran√ßois Chollet, [MIT License](https://github.com/keras-team/keras/blob/master/LICENSE). This version (c) 2018 Fabian Offert, [MIT License](LICENSE)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ge1CpIrxq_3",
        "colab_type": "text"
      },
      "source": [
        "## Background\n",
        "\n",
        "Feature visualization has been an important area of research within machine learning in general and deep learning in particular at least since 2014 [Zeiler 2014, Simonyan 2014]. \"Deep Dream\", for instance, works by applying feature visualization techniques to images, albeit optimized for producing the kind of visuals it has become famous for. Since then, particularly with the invention of GANs, more elaborate methods have emerged that employ natural image priors to \"bias\" visualizations towards more \"legible\" images [Dosovitskiy 2016, Nguyen 2016a, Nguyen 206b, Nguyen 2017]. Recently, feature visualization and related methods have received a lot of attention as possible solutions to the problem of interpretability, most prominently in [Olah 2017, Olah 2018]. Nevertheless, almost all visualization methods rely on the principle of activation maximization. They visualize the learned features of a particular neuron/channel/layer by optimizing an input image to maximally activate this neuron/channel/layer. \n",
        "\n",
        "Below, we visualize the features of selected channels from the InceptionV1 (also known as GoogLeNet) network, trained on ImageNet. As [Olah 2018] point out, this particular network seems to produce much more legible visualizations then comparable (newer) networks, even without supplying natural image priors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gg47SOawxq_8",
        "colab_type": "text"
      },
      "source": [
        "## Imports\n",
        "\n",
        "We are importing almost the same libraries as in the [\"Deep Dreaming\" notebook](2-deepdream.ipynb), except for two filter functions from SciPy, and two libraries to interface with the operating system. We need these later to run ImageMagick on our images to produce a nice montage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HM8qwyIExq_-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "78dac0a0-5dcd-491c-f8bf-e60233e70b1f"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "import numpy as np\n",
        "import scipy\n",
        "from scipy.ndimage.filters import gaussian_filter, median_filter\n",
        "from keras import backend as K\n",
        "from io import BytesIO\n",
        "import PIL.Image\n",
        "from IPython.display import clear_output, Image, display\n",
        "from subprocess import call\n",
        "import os\n",
        "import imageio"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQeoxC4XxrAG",
        "colab_type": "text"
      },
      "source": [
        "## Settings\n",
        "\n",
        "We use InceptionV1 as our model. As this architecture does not ship with Keras, we utilize [this custom implementation](https://github.com/fchollet/deep-learning-models/pull/59), with some minor changes/fixes. Most importantly, we change the softmax activation function into a linear activation function, as suggested in [Simonyan 2014]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nki02cxx4VQ",
        "colab_type": "code",
        "outputId": "f51e88db-37bb-4206-86a9-a454a92a3520",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from google.colab import files\n",
        "src = list(files.upload().values())[0]\n",
        "open('inception_v1_linear.py','wb').write(src)\n",
        "import inception_v1_linear"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e8d0475c-7eab-4a90-9add-eff0949f731b\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-e8d0475c-7eab-4a90-9add-eff0949f731b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving inception_v1_linear.py to inception_v1_linear.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0wVzTh5xrAI",
        "colab_type": "code",
        "outputId": "6c0376ec-17e8-41b3-e693-4e70411c1279",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# We are in the \"test\" phase, not the \"training\" phase, w.r.t. to the model we are analyzing\n",
        "K.set_learning_phase(0)\n",
        "\n",
        "# Load model with ImageNet pre-trained weights\n",
        "# Source: https://github.com/fchollet/deep-learning-models/pull/59\n",
        "# import inception_v1_linear\n",
        "model = inception_v1_linear.InceptionV1(weights='imagenet', include_top=True) \n",
        "model.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "Downloading data from http://redcatlabs.com/downloads/inception_v1_weights_tf_dim_ordering_tf_kernels.h5\n",
            "26968064/26962860 [==============================] - 1s 0us/step\n",
            "Model: \"inception_v1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Conv2d_1a_7x7_conv (Conv2D)     (None, 112, 112, 64) 9408        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "Conv2d_1a_7x7_bn (BatchNormaliz (None, 112, 112, 64) 192         Conv2d_1a_7x7_conv[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "Conv2d_1a_7x7_act (Activation)  (None, 112, 112, 64) 0           Conv2d_1a_7x7_bn[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "MaxPool_2a_3x3 (MaxPooling2D)   (None, 56, 56, 64)   0           Conv2d_1a_7x7_act[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Conv2d_2b_1x1_conv (Conv2D)     (None, 56, 56, 64)   4096        MaxPool_2a_3x3[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Conv2d_2b_1x1_bn (BatchNormaliz (None, 56, 56, 64)   192         Conv2d_2b_1x1_conv[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "Conv2d_2b_1x1_act (Activation)  (None, 56, 56, 64)   0           Conv2d_2b_1x1_bn[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "Conv2d_2c_3x3_conv (Conv2D)     (None, 56, 56, 192)  110592      Conv2d_2b_1x1_act[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Conv2d_2c_3x3_bn (BatchNormaliz (None, 56, 56, 192)  576         Conv2d_2c_3x3_conv[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "Conv2d_2c_3x3_act (Activation)  (None, 56, 56, 192)  0           Conv2d_2c_3x3_bn[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "MaxPool_3a_3x3 (MaxPooling2D)   (None, 28, 28, 192)  0           Conv2d_2c_3x3_act[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3b_Branch_1_a_1x1_conv (C (None, 28, 28, 96)   18432       MaxPool_3a_3x3[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3b_Branch_2_a_1x1_conv (C (None, 28, 28, 16)   3072        MaxPool_3a_3x3[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3b_Branch_1_a_1x1_bn (Bat (None, 28, 28, 96)   288         Mixed_3b_Branch_1_a_1x1_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3b_Branch_2_a_1x1_bn (Bat (None, 28, 28, 16)   48          Mixed_3b_Branch_2_a_1x1_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3b_Branch_1_a_1x1_act (Ac (None, 28, 28, 96)   0           Mixed_3b_Branch_1_a_1x1_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3b_Branch_2_a_1x1_act (Ac (None, 28, 28, 16)   0           Mixed_3b_Branch_2_a_1x1_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3b_Branch_3_a_max (MaxPoo (None, 28, 28, 192)  0           MaxPool_3a_3x3[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3b_Branch_0_a_1x1_conv (C (None, 28, 28, 64)   12288       MaxPool_3a_3x3[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3b_Branch_1_b_3x3_conv (C (None, 28, 28, 128)  110592      Mixed_3b_Branch_1_a_1x1_act[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3b_Branch_2_b_3x3_conv (C (None, 28, 28, 32)   4608        Mixed_3b_Branch_2_a_1x1_act[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3b_Branch_3_b_1x1_conv (C (None, 28, 28, 32)   6144        Mixed_3b_Branch_3_a_max[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3b_Branch_0_a_1x1_bn (Bat (None, 28, 28, 64)   192         Mixed_3b_Branch_0_a_1x1_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3b_Branch_1_b_3x3_bn (Bat (None, 28, 28, 128)  384         Mixed_3b_Branch_1_b_3x3_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3b_Branch_2_b_3x3_bn (Bat (None, 28, 28, 32)   96          Mixed_3b_Branch_2_b_3x3_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3b_Branch_3_b_1x1_bn (Bat (None, 28, 28, 32)   96          Mixed_3b_Branch_3_b_1x1_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3b_Branch_0_a_1x1_act (Ac (None, 28, 28, 64)   0           Mixed_3b_Branch_0_a_1x1_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3b_Branch_1_b_3x3_act (Ac (None, 28, 28, 128)  0           Mixed_3b_Branch_1_b_3x3_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3b_Branch_2_b_3x3_act (Ac (None, 28, 28, 32)   0           Mixed_3b_Branch_2_b_3x3_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3b_Branch_3_b_1x1_act (Ac (None, 28, 28, 32)   0           Mixed_3b_Branch_3_b_1x1_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3b_Concatenated (Concaten (None, 28, 28, 256)  0           Mixed_3b_Branch_0_a_1x1_act[0][0]\n",
            "                                                                 Mixed_3b_Branch_1_b_3x3_act[0][0]\n",
            "                                                                 Mixed_3b_Branch_2_b_3x3_act[0][0]\n",
            "                                                                 Mixed_3b_Branch_3_b_1x1_act[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3c_Branch_1_a_1x1_conv (C (None, 28, 28, 128)  32768       Mixed_3b_Concatenated[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3c_Branch_2_a_1x1_conv (C (None, 28, 28, 32)   8192        Mixed_3b_Concatenated[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3c_Branch_1_a_1x1_bn (Bat (None, 28, 28, 128)  384         Mixed_3c_Branch_1_a_1x1_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3c_Branch_2_a_1x1_bn (Bat (None, 28, 28, 32)   96          Mixed_3c_Branch_2_a_1x1_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3c_Branch_1_a_1x1_act (Ac (None, 28, 28, 128)  0           Mixed_3c_Branch_1_a_1x1_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3c_Branch_2_a_1x1_act (Ac (None, 28, 28, 32)   0           Mixed_3c_Branch_2_a_1x1_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3c_Branch_3_a_max (MaxPoo (None, 28, 28, 256)  0           Mixed_3b_Concatenated[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3c_Branch_0_a_1x1_conv (C (None, 28, 28, 128)  32768       Mixed_3b_Concatenated[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3c_Branch_1_b_3x3_conv (C (None, 28, 28, 192)  221184      Mixed_3c_Branch_1_a_1x1_act[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3c_Branch_2_b_3x3_conv (C (None, 28, 28, 96)   27648       Mixed_3c_Branch_2_a_1x1_act[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3c_Branch_3_b_1x1_conv (C (None, 28, 28, 64)   16384       Mixed_3c_Branch_3_a_max[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3c_Branch_0_a_1x1_bn (Bat (None, 28, 28, 128)  384         Mixed_3c_Branch_0_a_1x1_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3c_Branch_1_b_3x3_bn (Bat (None, 28, 28, 192)  576         Mixed_3c_Branch_1_b_3x3_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3c_Branch_2_b_3x3_bn (Bat (None, 28, 28, 96)   288         Mixed_3c_Branch_2_b_3x3_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3c_Branch_3_b_1x1_bn (Bat (None, 28, 28, 64)   192         Mixed_3c_Branch_3_b_1x1_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3c_Branch_0_a_1x1_act (Ac (None, 28, 28, 128)  0           Mixed_3c_Branch_0_a_1x1_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3c_Branch_1_b_3x3_act (Ac (None, 28, 28, 192)  0           Mixed_3c_Branch_1_b_3x3_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3c_Branch_2_b_3x3_act (Ac (None, 28, 28, 96)   0           Mixed_3c_Branch_2_b_3x3_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3c_Branch_3_b_1x1_act (Ac (None, 28, 28, 64)   0           Mixed_3c_Branch_3_b_1x1_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_3c_Concatenated (Concaten (None, 28, 28, 480)  0           Mixed_3c_Branch_0_a_1x1_act[0][0]\n",
            "                                                                 Mixed_3c_Branch_1_b_3x3_act[0][0]\n",
            "                                                                 Mixed_3c_Branch_2_b_3x3_act[0][0]\n",
            "                                                                 Mixed_3c_Branch_3_b_1x1_act[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "MaxPool_4a_3x3 (MaxPooling2D)   (None, 14, 14, 480)  0           Mixed_3c_Concatenated[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4b_Branch_1_a_1x1_conv (C (None, 14, 14, 96)   46080       MaxPool_4a_3x3[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4b_Branch_2_a_1x1_conv (C (None, 14, 14, 16)   7680        MaxPool_4a_3x3[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4b_Branch_1_a_1x1_bn (Bat (None, 14, 14, 96)   288         Mixed_4b_Branch_1_a_1x1_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4b_Branch_2_a_1x1_bn (Bat (None, 14, 14, 16)   48          Mixed_4b_Branch_2_a_1x1_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4b_Branch_1_a_1x1_act (Ac (None, 14, 14, 96)   0           Mixed_4b_Branch_1_a_1x1_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4b_Branch_2_a_1x1_act (Ac (None, 14, 14, 16)   0           Mixed_4b_Branch_2_a_1x1_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4b_Branch_3_a_max (MaxPoo (None, 14, 14, 480)  0           MaxPool_4a_3x3[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4b_Branch_0_a_1x1_conv (C (None, 14, 14, 192)  92160       MaxPool_4a_3x3[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4b_Branch_1_b_3x3_conv (C (None, 14, 14, 208)  179712      Mixed_4b_Branch_1_a_1x1_act[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4b_Branch_2_b_3x3_conv (C (None, 14, 14, 48)   6912        Mixed_4b_Branch_2_a_1x1_act[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4b_Branch_3_b_1x1_conv (C (None, 14, 14, 64)   30720       Mixed_4b_Branch_3_a_max[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4b_Branch_0_a_1x1_bn (Bat (None, 14, 14, 192)  576         Mixed_4b_Branch_0_a_1x1_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4b_Branch_1_b_3x3_bn (Bat (None, 14, 14, 208)  624         Mixed_4b_Branch_1_b_3x3_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4b_Branch_2_b_3x3_bn (Bat (None, 14, 14, 48)   144         Mixed_4b_Branch_2_b_3x3_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4b_Branch_3_b_1x1_bn (Bat (None, 14, 14, 64)   192         Mixed_4b_Branch_3_b_1x1_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4b_Branch_0_a_1x1_act (Ac (None, 14, 14, 192)  0           Mixed_4b_Branch_0_a_1x1_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4b_Branch_1_b_3x3_act (Ac (None, 14, 14, 208)  0           Mixed_4b_Branch_1_b_3x3_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4b_Branch_2_b_3x3_act (Ac (None, 14, 14, 48)   0           Mixed_4b_Branch_2_b_3x3_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4b_Branch_3_b_1x1_act (Ac (None, 14, 14, 64)   0           Mixed_4b_Branch_3_b_1x1_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4b_Concatenated (Concaten (None, 14, 14, 512)  0           Mixed_4b_Branch_0_a_1x1_act[0][0]\n",
            "                                                                 Mixed_4b_Branch_1_b_3x3_act[0][0]\n",
            "                                                                 Mixed_4b_Branch_2_b_3x3_act[0][0]\n",
            "                                                                 Mixed_4b_Branch_3_b_1x1_act[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4c_Branch_1_a_1x1_conv (C (None, 14, 14, 112)  57344       Mixed_4b_Concatenated[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4c_Branch_2_a_1x1_conv (C (None, 14, 14, 24)   12288       Mixed_4b_Concatenated[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4c_Branch_1_a_1x1_bn (Bat (None, 14, 14, 112)  336         Mixed_4c_Branch_1_a_1x1_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4c_Branch_2_a_1x1_bn (Bat (None, 14, 14, 24)   72          Mixed_4c_Branch_2_a_1x1_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4c_Branch_1_a_1x1_act (Ac (None, 14, 14, 112)  0           Mixed_4c_Branch_1_a_1x1_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4c_Branch_2_a_1x1_act (Ac (None, 14, 14, 24)   0           Mixed_4c_Branch_2_a_1x1_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4c_Branch_3_a_max (MaxPoo (None, 14, 14, 512)  0           Mixed_4b_Concatenated[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4c_Branch_0_a_1x1_conv (C (None, 14, 14, 160)  81920       Mixed_4b_Concatenated[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4c_Branch_1_b_3x3_conv (C (None, 14, 14, 224)  225792      Mixed_4c_Branch_1_a_1x1_act[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4c_Branch_2_b_3x3_conv (C (None, 14, 14, 64)   13824       Mixed_4c_Branch_2_a_1x1_act[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4c_Branch_3_b_1x1_conv (C (None, 14, 14, 64)   32768       Mixed_4c_Branch_3_a_max[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4c_Branch_0_a_1x1_bn (Bat (None, 14, 14, 160)  480         Mixed_4c_Branch_0_a_1x1_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4c_Branch_1_b_3x3_bn (Bat (None, 14, 14, 224)  672         Mixed_4c_Branch_1_b_3x3_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4c_Branch_2_b_3x3_bn (Bat (None, 14, 14, 64)   192         Mixed_4c_Branch_2_b_3x3_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4c_Branch_3_b_1x1_bn (Bat (None, 14, 14, 64)   192         Mixed_4c_Branch_3_b_1x1_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4c_Branch_0_a_1x1_act (Ac (None, 14, 14, 160)  0           Mixed_4c_Branch_0_a_1x1_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4c_Branch_1_b_3x3_act (Ac (None, 14, 14, 224)  0           Mixed_4c_Branch_1_b_3x3_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4c_Branch_2_b_3x3_act (Ac (None, 14, 14, 64)   0           Mixed_4c_Branch_2_b_3x3_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4c_Branch_3_b_1x1_act (Ac (None, 14, 14, 64)   0           Mixed_4c_Branch_3_b_1x1_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4c_Concatenated (Concaten (None, 14, 14, 512)  0           Mixed_4c_Branch_0_a_1x1_act[0][0]\n",
            "                                                                 Mixed_4c_Branch_1_b_3x3_act[0][0]\n",
            "                                                                 Mixed_4c_Branch_2_b_3x3_act[0][0]\n",
            "                                                                 Mixed_4c_Branch_3_b_1x1_act[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4d_Branch_1_a_1x1_conv (C (None, 14, 14, 128)  65536       Mixed_4c_Concatenated[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4d_Branch_2_a_1x1_conv (C (None, 14, 14, 24)   12288       Mixed_4c_Concatenated[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4d_Branch_1_a_1x1_bn (Bat (None, 14, 14, 128)  384         Mixed_4d_Branch_1_a_1x1_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4d_Branch_2_a_1x1_bn (Bat (None, 14, 14, 24)   72          Mixed_4d_Branch_2_a_1x1_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4d_Branch_1_a_1x1_act (Ac (None, 14, 14, 128)  0           Mixed_4d_Branch_1_a_1x1_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4d_Branch_2_a_1x1_act (Ac (None, 14, 14, 24)   0           Mixed_4d_Branch_2_a_1x1_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4d_Branch_3_a_max (MaxPoo (None, 14, 14, 512)  0           Mixed_4c_Concatenated[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4d_Branch_0_a_1x1_conv (C (None, 14, 14, 128)  65536       Mixed_4c_Concatenated[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4d_Branch_1_b_3x3_conv (C (None, 14, 14, 256)  294912      Mixed_4d_Branch_1_a_1x1_act[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4d_Branch_2_b_3x3_conv (C (None, 14, 14, 64)   13824       Mixed_4d_Branch_2_a_1x1_act[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4d_Branch_3_b_1x1_conv (C (None, 14, 14, 64)   32768       Mixed_4d_Branch_3_a_max[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4d_Branch_0_a_1x1_bn (Bat (None, 14, 14, 128)  384         Mixed_4d_Branch_0_a_1x1_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4d_Branch_1_b_3x3_bn (Bat (None, 14, 14, 256)  768         Mixed_4d_Branch_1_b_3x3_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4d_Branch_2_b_3x3_bn (Bat (None, 14, 14, 64)   192         Mixed_4d_Branch_2_b_3x3_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4d_Branch_3_b_1x1_bn (Bat (None, 14, 14, 64)   192         Mixed_4d_Branch_3_b_1x1_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4d_Branch_0_a_1x1_act (Ac (None, 14, 14, 128)  0           Mixed_4d_Branch_0_a_1x1_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4d_Branch_1_b_3x3_act (Ac (None, 14, 14, 256)  0           Mixed_4d_Branch_1_b_3x3_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4d_Branch_2_b_3x3_act (Ac (None, 14, 14, 64)   0           Mixed_4d_Branch_2_b_3x3_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4d_Branch_3_b_1x1_act (Ac (None, 14, 14, 64)   0           Mixed_4d_Branch_3_b_1x1_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4d_Concatenated (Concaten (None, 14, 14, 512)  0           Mixed_4d_Branch_0_a_1x1_act[0][0]\n",
            "                                                                 Mixed_4d_Branch_1_b_3x3_act[0][0]\n",
            "                                                                 Mixed_4d_Branch_2_b_3x3_act[0][0]\n",
            "                                                                 Mixed_4d_Branch_3_b_1x1_act[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4e_Branch_1_a_1x1_conv (C (None, 14, 14, 144)  73728       Mixed_4d_Concatenated[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4e_Branch_2_a_1x1_conv (C (None, 14, 14, 32)   16384       Mixed_4d_Concatenated[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4e_Branch_1_a_1x1_bn (Bat (None, 14, 14, 144)  432         Mixed_4e_Branch_1_a_1x1_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4e_Branch_2_a_1x1_bn (Bat (None, 14, 14, 32)   96          Mixed_4e_Branch_2_a_1x1_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4e_Branch_1_a_1x1_act (Ac (None, 14, 14, 144)  0           Mixed_4e_Branch_1_a_1x1_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4e_Branch_2_a_1x1_act (Ac (None, 14, 14, 32)   0           Mixed_4e_Branch_2_a_1x1_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4e_Branch_3_a_max (MaxPoo (None, 14, 14, 512)  0           Mixed_4d_Concatenated[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4e_Branch_0_a_1x1_conv (C (None, 14, 14, 112)  57344       Mixed_4d_Concatenated[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4e_Branch_1_b_3x3_conv (C (None, 14, 14, 288)  373248      Mixed_4e_Branch_1_a_1x1_act[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4e_Branch_2_b_3x3_conv (C (None, 14, 14, 64)   18432       Mixed_4e_Branch_2_a_1x1_act[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4e_Branch_3_b_1x1_conv (C (None, 14, 14, 64)   32768       Mixed_4e_Branch_3_a_max[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4e_Branch_0_a_1x1_bn (Bat (None, 14, 14, 112)  336         Mixed_4e_Branch_0_a_1x1_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4e_Branch_1_b_3x3_bn (Bat (None, 14, 14, 288)  864         Mixed_4e_Branch_1_b_3x3_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4e_Branch_2_b_3x3_bn (Bat (None, 14, 14, 64)   192         Mixed_4e_Branch_2_b_3x3_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4e_Branch_3_b_1x1_bn (Bat (None, 14, 14, 64)   192         Mixed_4e_Branch_3_b_1x1_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4e_Branch_0_a_1x1_act (Ac (None, 14, 14, 112)  0           Mixed_4e_Branch_0_a_1x1_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4e_Branch_1_b_3x3_act (Ac (None, 14, 14, 288)  0           Mixed_4e_Branch_1_b_3x3_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4e_Branch_2_b_3x3_act (Ac (None, 14, 14, 64)   0           Mixed_4e_Branch_2_b_3x3_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4e_Branch_3_b_1x1_act (Ac (None, 14, 14, 64)   0           Mixed_4e_Branch_3_b_1x1_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4e_Concatenated (Concaten (None, 14, 14, 528)  0           Mixed_4e_Branch_0_a_1x1_act[0][0]\n",
            "                                                                 Mixed_4e_Branch_1_b_3x3_act[0][0]\n",
            "                                                                 Mixed_4e_Branch_2_b_3x3_act[0][0]\n",
            "                                                                 Mixed_4e_Branch_3_b_1x1_act[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4f_Branch_1_a_1x1_conv (C (None, 14, 14, 160)  84480       Mixed_4e_Concatenated[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4f_Branch_2_a_1x1_conv (C (None, 14, 14, 32)   16896       Mixed_4e_Concatenated[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4f_Branch_1_a_1x1_bn (Bat (None, 14, 14, 160)  480         Mixed_4f_Branch_1_a_1x1_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4f_Branch_2_a_1x1_bn (Bat (None, 14, 14, 32)   96          Mixed_4f_Branch_2_a_1x1_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4f_Branch_1_a_1x1_act (Ac (None, 14, 14, 160)  0           Mixed_4f_Branch_1_a_1x1_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4f_Branch_2_a_1x1_act (Ac (None, 14, 14, 32)   0           Mixed_4f_Branch_2_a_1x1_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4f_Branch_3_a_max (MaxPoo (None, 14, 14, 528)  0           Mixed_4e_Concatenated[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4f_Branch_0_a_1x1_conv (C (None, 14, 14, 256)  135168      Mixed_4e_Concatenated[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4f_Branch_1_b_3x3_conv (C (None, 14, 14, 320)  460800      Mixed_4f_Branch_1_a_1x1_act[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4f_Branch_2_b_3x3_conv (C (None, 14, 14, 128)  36864       Mixed_4f_Branch_2_a_1x1_act[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4f_Branch_3_b_1x1_conv (C (None, 14, 14, 128)  67584       Mixed_4f_Branch_3_a_max[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4f_Branch_0_a_1x1_bn (Bat (None, 14, 14, 256)  768         Mixed_4f_Branch_0_a_1x1_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4f_Branch_1_b_3x3_bn (Bat (None, 14, 14, 320)  960         Mixed_4f_Branch_1_b_3x3_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4f_Branch_2_b_3x3_bn (Bat (None, 14, 14, 128)  384         Mixed_4f_Branch_2_b_3x3_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4f_Branch_3_b_1x1_bn (Bat (None, 14, 14, 128)  384         Mixed_4f_Branch_3_b_1x1_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4f_Branch_0_a_1x1_act (Ac (None, 14, 14, 256)  0           Mixed_4f_Branch_0_a_1x1_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4f_Branch_1_b_3x3_act (Ac (None, 14, 14, 320)  0           Mixed_4f_Branch_1_b_3x3_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4f_Branch_2_b_3x3_act (Ac (None, 14, 14, 128)  0           Mixed_4f_Branch_2_b_3x3_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4f_Branch_3_b_1x1_act (Ac (None, 14, 14, 128)  0           Mixed_4f_Branch_3_b_1x1_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_4f_Concatenated (Concaten (None, 14, 14, 832)  0           Mixed_4f_Branch_0_a_1x1_act[0][0]\n",
            "                                                                 Mixed_4f_Branch_1_b_3x3_act[0][0]\n",
            "                                                                 Mixed_4f_Branch_2_b_3x3_act[0][0]\n",
            "                                                                 Mixed_4f_Branch_3_b_1x1_act[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "MaxPool_5a_2x2 (MaxPooling2D)   (None, 7, 7, 832)    0           Mixed_4f_Concatenated[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5b_Branch_1_a_1x1_conv (C (None, 7, 7, 160)    133120      MaxPool_5a_2x2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5b_Branch_2_a_1x1_conv (C (None, 7, 7, 32)     26624       MaxPool_5a_2x2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5b_Branch_1_a_1x1_bn (Bat (None, 7, 7, 160)    480         Mixed_5b_Branch_1_a_1x1_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5b_Branch_2_a_1x1_bn (Bat (None, 7, 7, 32)     96          Mixed_5b_Branch_2_a_1x1_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5b_Branch_1_a_1x1_act (Ac (None, 7, 7, 160)    0           Mixed_5b_Branch_1_a_1x1_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5b_Branch_2_a_1x1_act (Ac (None, 7, 7, 32)     0           Mixed_5b_Branch_2_a_1x1_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5b_Branch_3_a_max (MaxPoo (None, 7, 7, 832)    0           MaxPool_5a_2x2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5b_Branch_0_a_1x1_conv (C (None, 7, 7, 256)    212992      MaxPool_5a_2x2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5b_Branch_1_b_3x3_conv (C (None, 7, 7, 320)    460800      Mixed_5b_Branch_1_a_1x1_act[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5b_Branch_2_b_3x3_conv (C (None, 7, 7, 128)    36864       Mixed_5b_Branch_2_a_1x1_act[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5b_Branch_3_b_1x1_conv (C (None, 7, 7, 128)    106496      Mixed_5b_Branch_3_a_max[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5b_Branch_0_a_1x1_bn (Bat (None, 7, 7, 256)    768         Mixed_5b_Branch_0_a_1x1_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5b_Branch_1_b_3x3_bn (Bat (None, 7, 7, 320)    960         Mixed_5b_Branch_1_b_3x3_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5b_Branch_2_b_3x3_bn (Bat (None, 7, 7, 128)    384         Mixed_5b_Branch_2_b_3x3_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5b_Branch_3_b_1x1_bn (Bat (None, 7, 7, 128)    384         Mixed_5b_Branch_3_b_1x1_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5b_Branch_0_a_1x1_act (Ac (None, 7, 7, 256)    0           Mixed_5b_Branch_0_a_1x1_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5b_Branch_1_b_3x3_act (Ac (None, 7, 7, 320)    0           Mixed_5b_Branch_1_b_3x3_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5b_Branch_2_b_3x3_act (Ac (None, 7, 7, 128)    0           Mixed_5b_Branch_2_b_3x3_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5b_Branch_3_b_1x1_act (Ac (None, 7, 7, 128)    0           Mixed_5b_Branch_3_b_1x1_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5b_Concatenated (Concaten (None, 7, 7, 832)    0           Mixed_5b_Branch_0_a_1x1_act[0][0]\n",
            "                                                                 Mixed_5b_Branch_1_b_3x3_act[0][0]\n",
            "                                                                 Mixed_5b_Branch_2_b_3x3_act[0][0]\n",
            "                                                                 Mixed_5b_Branch_3_b_1x1_act[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5c_Branch_1_a_1x1_conv (C (None, 7, 7, 192)    159744      Mixed_5b_Concatenated[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5c_Branch_2_a_1x1_conv (C (None, 7, 7, 48)     39936       Mixed_5b_Concatenated[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5c_Branch_1_a_1x1_bn (Bat (None, 7, 7, 192)    576         Mixed_5c_Branch_1_a_1x1_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5c_Branch_2_a_1x1_bn (Bat (None, 7, 7, 48)     144         Mixed_5c_Branch_2_a_1x1_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5c_Branch_1_a_1x1_act (Ac (None, 7, 7, 192)    0           Mixed_5c_Branch_1_a_1x1_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5c_Branch_2_a_1x1_act (Ac (None, 7, 7, 48)     0           Mixed_5c_Branch_2_a_1x1_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5c_Branch_3_a_max (MaxPoo (None, 7, 7, 832)    0           Mixed_5b_Concatenated[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5c_Branch_0_a_1x1_conv (C (None, 7, 7, 384)    319488      Mixed_5b_Concatenated[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5c_Branch_1_b_3x3_conv (C (None, 7, 7, 384)    663552      Mixed_5c_Branch_1_a_1x1_act[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5c_Branch_2_b_3x3_conv (C (None, 7, 7, 128)    55296       Mixed_5c_Branch_2_a_1x1_act[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5c_Branch_3_b_1x1_conv (C (None, 7, 7, 128)    106496      Mixed_5c_Branch_3_a_max[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5c_Branch_0_a_1x1_bn (Bat (None, 7, 7, 384)    1152        Mixed_5c_Branch_0_a_1x1_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5c_Branch_1_b_3x3_bn (Bat (None, 7, 7, 384)    1152        Mixed_5c_Branch_1_b_3x3_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5c_Branch_2_b_3x3_bn (Bat (None, 7, 7, 128)    384         Mixed_5c_Branch_2_b_3x3_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5c_Branch_3_b_1x1_bn (Bat (None, 7, 7, 128)    384         Mixed_5c_Branch_3_b_1x1_conv[0][0\n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5c_Branch_0_a_1x1_act (Ac (None, 7, 7, 384)    0           Mixed_5c_Branch_0_a_1x1_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5c_Branch_1_b_3x3_act (Ac (None, 7, 7, 384)    0           Mixed_5c_Branch_1_b_3x3_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5c_Branch_2_b_3x3_act (Ac (None, 7, 7, 128)    0           Mixed_5c_Branch_2_b_3x3_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5c_Branch_3_b_1x1_act (Ac (None, 7, 7, 128)    0           Mixed_5c_Branch_3_b_1x1_bn[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Mixed_5c_Concatenated (Concaten (None, 7, 7, 1024)   0           Mixed_5c_Branch_0_a_1x1_act[0][0]\n",
            "                                                                 Mixed_5c_Branch_1_b_3x3_act[0][0]\n",
            "                                                                 Mixed_5c_Branch_2_b_3x3_act[0][0]\n",
            "                                                                 Mixed_5c_Branch_3_b_1x1_act[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 1, 1, 1024)   0           Mixed_5c_Concatenated[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 1, 1, 1024)   0           average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "Logits (Conv2D)                 (None, 1, 1, 1001)   1026025     dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "Logits_flat (Flatten)           (None, 1001)         0           Logits[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "Predictions (Activation)        (None, 1001)         0           Logits_flat[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 6,633,209\n",
            "Trainable params: 6,618,649\n",
            "Non-trainable params: 14,560\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBHin2EMxrAP",
        "colab_type": "text"
      },
      "source": [
        "We also define two sets of settings: one for visualizing the classes of the model (`settings_InceptionV1_classes`), and one for visualizing arbitrary layers of the model (`settings_InceptionV1_single`). Other than with V3, for InceptionV1 the input size is fixed to $224^2$ pixels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBKlqc1pxrAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "settings_InceptionV1_classes = ['Predictions']\n",
        "settings_InceptionV1_single = ['Mixed_4b_Concatenated']\n",
        "settings = settings_InceptionV1_classes\n",
        "size = 224 # 224 for InceptionV1, variable for InceptionV3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDqVk1sIxrAU",
        "colab_type": "text"
      },
      "source": [
        "Finally, we define which part of the layer we would like to visualize: just one particular channel (e.g. `filters = [8]` would look at the \"hen\" class in the `Predictions` layer), all available channels (`filters = None; sum_filters = False`), or the sum of all available channels (`filters = None; sum_filters = True`). If we are analyzing the output layer, the classes are defined according to [this list](https://github.com/happynear/caffe-windows/blob/master/examples/GoogLeNet/synset_words.txt)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8cV4XvDxrAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# filters = [605, 605, 605, 605, 605, 605, 605, 605, 605, 605] # If None use all available filters (don't use None for prediction layers, define range!)\n",
        "filters = [351]\n",
        "sum_filters = False # If true, sum all filters, if false iterate over all filters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4CY7DRRxrAY",
        "colab_type": "text"
      },
      "source": [
        "## Image preprocessing and deprocessing\n",
        "\n",
        "We are using the same image helper functions as in the [\"Deep Dreaming\" notebook](3-deepdream.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqK6qwiYxrAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_image(image_path):\n",
        "    img = load_img(image_path)\n",
        "    img = img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0) # Add extra dimension for mini batches (not used)\n",
        "    img = inception_v1_linear.preprocess_input(img) # 3D -> 1D\n",
        "    return img\n",
        "\n",
        "def deprocess_image(x):\n",
        "    # Check ~/.keras/keras.json to make sure \"image_data_format\" is set to \"channels_last\"\n",
        "    # or print(K.image_data_format())\n",
        "    x = x.reshape((x.shape[1], x.shape[2], 3)) # \"Remove\" extra dimension, channels last\n",
        "    x /= 2.\n",
        "    x += 0.5\n",
        "    x *= 255.\n",
        "    x = np.clip(x, 0, 255).astype('uint8') # Clip to visible range\n",
        "    x = autotone(x)\n",
        "    return x\n",
        "\n",
        "# Simple resize function based on scipy\n",
        "def resize_img(img, size):\n",
        "    img = np.copy(img)\n",
        "    factors = (1, float(size[0]) / img.shape[1], float(size[1]) / img.shape[2], 1)\n",
        "    return scipy.ndimage.zoom(img, factors, order=1)\n",
        "\n",
        "# Simple save function based on scipy\n",
        "def save_image(img, fname):\n",
        "    pil_img = deprocess_image(np.copy(img))\n",
        "    imageio.imwrite(fname, pil_img)\n",
        "    # scipy.misc.imsave(fname, pil_img)\n",
        "    \n",
        "def show_image(img, fmt='jpeg'):\n",
        "    img = deprocess_image(np.copy(img))\n",
        "    f = BytesIO()\n",
        "    PIL.Image.fromarray(img).save(f, fmt)\n",
        "    display(Image(data=f.getvalue()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "868Sdv69xrAd",
        "colab_type": "text"
      },
      "source": [
        "## More image preprocessing and deprocessing \n",
        "\n",
        "In addition, we define two functions to save images sequences. The first one (`save_image_numbered`) simly creates numbered sequences, the second one (`save_image_sweep`) includes a dictionary into the filename. We use this second function ofr hyperparameter sweeps (see below)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2WDLRoyxrAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def montage_images(folder, size):\n",
        "    print('Montaging...')\n",
        "    geometry = '-geometry \"' + str(size) + 'x' + str(size) + '+2+2>\" '\n",
        "    label = '-label \"%t\" '\n",
        "    output = folder + '/montage.jpg'\n",
        "    infiles = folder + '/*.jpg '\n",
        "    montage = 'montage  ' + label + infiles + geometry + output\n",
        "    call(montage, shell=True)\n",
        "    print('...done.')\n",
        "\n",
        "def save_image_numbered(img, nr, folder):\n",
        "    f = '{0:03d}'.format(nr)\n",
        "    p = folder + '/' + f + '.jpg'\n",
        "    save_image(img, p)\n",
        "    \n",
        "def save_image_sweep(img, filter, sweep, folder):\n",
        "    # Concatenate the list of values in the dictionary as strings\n",
        "    f = str(filter) + '_' + '_'.join(str(x) for x in list(sweep.values()))\n",
        "    p = folder + '/' + f + '.jpg'\n",
        "    print('Writing ' + p)\n",
        "    save_image(img, p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Anc0dmRxrAh",
        "colab_type": "text"
      },
      "source": [
        "## Regularization\n",
        "\n",
        "Regularization introduces priors into the loss function. By utilizing regularization, we end up with \"better\", more legible images. To start, we define a simple \"auto tone\" function (`autotone`) that normalizes each color channel in an image separately ‚Äì exactly what Photoshop is doing in its \"auto tone\" function ‚Äì to get more legible images. We also do not start with a plain gray image but with a gray image that includes some Gaussian white noise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkE-oX5jxrAi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Normalize each color sepearately (Photoshop auto tone)\n",
        "def autotone(img):\n",
        "    img[:,:,0] = np.interp(img[:,:,0], [np.amin(img[:,:,0]), np.amax(img[:,:,0])], [0, 255])\n",
        "    img[:,:,1] = np.interp(img[:,:,1], [np.amin(img[:,:,1]), np.amax(img[:,:,1])], [0, 255])\n",
        "    img[:,:,2] = np.interp(img[:,:,2], [np.amin(img[:,:,2]), np.amax(img[:,:,2])], [0, 255])\n",
        "    return img\n",
        "\n",
        "def gray_square(size, variance): \n",
        "    img = np.random.normal(0, variance, (1, size, size, 3))\n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfyxMPWqxrAk",
        "colab_type": "text"
      },
      "source": [
        "Three other regularization functions are embedded into the gradient ascent function: a Gaussian blur function, and a median filter function. Their utilization is controlled by several hyperparameters that define how often these filters are applied (every four iterations, for instance) and how strong they are. By tuning these hyperparameters, either manually or automatically by means of hyperparameter sweeps (see below), we can find settings that subjectively produce better images, images that are more obviously representations of existing concepts. As so often, however, good is better then best, as the \"best\" images, i.e. the images that activate the layer/filter we are looking at the most, are usually just high-frequency noise. As pointed out by [Szegedy 2013], this link between adversarial examples and semantic is one of the most \"intruiging properties\" of neural networks that has [many interesting epistemological implications](https://arxiv.org/abs/1711.08042)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjAHTqTcxrAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient_ascent(x, \n",
        "                    iterations=1000, \n",
        "                    step=0.01, \n",
        "                    max_loss=None, \n",
        "                    blur_std=0, \n",
        "                    blur_every=8, \n",
        "                    median_fsize=5, \n",
        "                    median_every=4):\n",
        "    \n",
        "    for i in range(iterations):\n",
        "        outs = fetch_loss_and_grads([x])\n",
        "        loss_value = outs[0]\n",
        "        grad_values = outs[1]\n",
        "        if max_loss is not None and loss_value > max_loss:\n",
        "            break\n",
        "        x += step * grad_values      \n",
        "        \n",
        "        if(i!=iterations-1): # No regularization on last iteration for good quality output \n",
        "            # Gaussian blur\n",
        "            if blur_std is not 0 and i % blur_every == 0 :\n",
        "                x = gaussian_filter(x, sigma=[0, blur_std, blur_std, 0])  \n",
        "            # Median filter\n",
        "            if median_fsize is not 0 and i % median_every == 0 :\n",
        "                x = median_filter(x, size=(1, median_fsize, median_fsize, 1))\n",
        "                \n",
        "    return x\n",
        "\n",
        "# Dictionary of the names of the layers and the layers themselves\n",
        "layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
        "\n",
        "# Input of the first layer, in the example: model.input\n",
        "dream = model.layers[0].input \n",
        "\n",
        "# A TF variable (persistent)\n",
        "loss = K.variable(0.)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OslgkZB-xrAn",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameter Sweeping\n",
        "\n",
        "To find the \"right\" hyperparameters for the regularization techniques that we have introduced, we create a \"sweep\" function that conveniently applies sets of pre-defined parameters in all possible permutations to the same class/filter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWOp5v2TxrAp",
        "colab_type": "code",
        "outputId": "62081594-b839-4a10-b6c6-81fffa82fc8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Sweep over all possible permutations of allowed hyperparameters in a very un-pythonic but simple way\n",
        "\n",
        "\"\"\"\n",
        "# Sweep InceptionV1\n",
        "iterations = [2500]\n",
        "steps = [0.01, 0.1]\n",
        "max_losses = [None]\n",
        "blur_stds = [0]\n",
        "blur_everys = [2, 4, 8]\n",
        "median_fsizes = [3, 5, 7]\n",
        "median_everys = [2, 4, 8]\n",
        "variances = [0.01, 1]\n",
        "\"\"\"\n",
        "\n",
        "# Best values InceptionV1 from sweep above\n",
        "iterations = [1000]\n",
        "steps = [0.01]\n",
        "max_losses = [None]\n",
        "blur_stds = [0]\n",
        "blur_everys = [4]\n",
        "median_fsizes = [5]\n",
        "median_everys = [4]\n",
        "variances = [0.01]\n",
        "\n",
        "sweeps = []\n",
        "\n",
        "for iteration in iterations:\n",
        "    for step in steps:\n",
        "        for max_loss in max_losses:\n",
        "            for blur_std in blur_stds:\n",
        "                for blur_every in blur_everys:\n",
        "                    for median_fsize in median_fsizes:\n",
        "                        for median_every in median_everys:\n",
        "                            for variance in variances:\n",
        "                                sweeps.append({'iterations':iteration,\n",
        "                                                'step':step,\n",
        "                                                'max_loss':max_loss,\n",
        "                                                'blur_std':blur_std,\n",
        "                                                'blur_every':blur_every,\n",
        "                                                'median_fsize':median_fsize,\n",
        "                                                'median_every':median_every,\n",
        "                                                'variance':variance})\n",
        "                                \n",
        "print(str(len(sweeps)) + ' sweeps generated')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 sweeps generated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTn3OnAYxrAw",
        "colab_type": "text"
      },
      "source": [
        "# Activation maximiation\n",
        "\n",
        "We now run the activation maximization process for the selected layers/channels. The process is exactly the same as in the [\"Deep Dreaming\" notebook](3-deepdream.ipynb), except for a simpler loss function if we are looking at classes, as the last layer already gives us a single scalar loss value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9QYQeSQxrAy",
        "colab_type": "code",
        "outputId": "f8162e7b-5d9b-4f87-c7ba-fce2cb1296f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "folder = '3-features/'\n",
        "\n",
        "# Iterate only over the layers picked above and their available filters\n",
        "for layer_name in settings: \n",
        "    assert layer_name in layer_dict.keys(), 'Layer ' + layer_name + ' not found in model.' # Layer in model?\n",
        "    \n",
        "    # Create directory to hold frames\n",
        "    if not os.path.exists(folder + layer_name):\n",
        "        os.makedirs(folder + layer_name)\n",
        "    \n",
        "    x = layer_dict[layer_name].output # Output of the current layer\n",
        "        \n",
        "    if (filters == None and not sum_filters): channels = list(range(x.shape[3])) # Iterate over all filters\n",
        "    elif (filters == None and sum_filters): channels = [1] # Sum all filters\n",
        "    else: channels = filters\n",
        "    \n",
        "    # We might want to stop early and not be stuck with the same filters every time\n",
        "    np.random.shuffle(channels)\n",
        "    \n",
        "    for channel in channels:\n",
        "        \n",
        "        for sweep in sweeps:\n",
        "    \n",
        "            # We avoid border artifacts by only involving non-border pixels in the loss, offset by 2 on all sides\n",
        "            if (filters == None and sum_filters): loss = K.sum(K.mean(x[:, 2: -2, 2: -2, :]))\n",
        "            elif (filters == None and not sum_filters): loss = K.sum(K.mean(x[:, 2: -2, 2: -2, channel]))\n",
        "            # Classification layers just give a single probability, so no sum/mean/offset\n",
        "            else: loss = model.layers[-1].output[0, channel] # Always output of the last layer\n",
        "    \n",
        "            # Compute the gradients of the dream w.r.t. the loss.\n",
        "            grads = K.gradients(loss, dream)[0]\n",
        "            # Normalize gradients.\n",
        "            grads /= K.maximum(K.mean(K.abs(grads)), K.epsilon())\n",
        "\n",
        "            fetch_loss_and_grads = K.function([dream], [loss, grads])\n",
        "\n",
        "            img = gray_square(size, sweep['variance'])\n",
        "            img = gradient_ascent(img, \n",
        "                                  iterations=sweep['iterations'], \n",
        "                                  step=sweep['step'], \n",
        "                                  max_loss=sweep['max_loss'],\n",
        "                                  blur_std=sweep['blur_std'], \n",
        "                                  blur_every=sweep['blur_every'], \n",
        "                                  median_fsize=sweep['median_fsize'], \n",
        "                                  median_every=sweep['median_every'])\n",
        "            if (len(sweeps) > 1): save_image_sweep(img, channel, sweep, '4-features/' + layer_name)\n",
        "            else:\n",
        "                show_image(img)\n",
        "                save_image_numbered(img, channel, folder + layer_name)\n",
        "        \n",
        "    montage_images(layer_name, size)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0a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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Montaging...\n",
            "...done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPv5JTwxxrA1",
        "colab_type": "text"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qdlnp1BxrA2",
        "colab_type": "text"
      },
      "source": [
        "### First 500 classes of InceptionV1 as per the hyperparameters defined above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RgWSlwlxrA3",
        "colab_type": "text"
      },
      "source": [
        "![](https://github.com/zentralwerkstatt/MAT594SP/blob/master/4-features/Predictions-500/montage.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbkaCQgGxrA4",
        "colab_type": "text"
      },
      "source": [
        "#### Some notable (hand-picked) classes\n",
        "Top to bottom: goldfish, Carassius auratus (2), loggerhead, loggerhead turtle, Caretta caretta (34), king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica (122), bee (310), bakery, bakeshop, bakehouse (416)\n",
        "\n",
        "\n",
        "![](https://github.com/zentralwerkstatt/MAT594SP/blob/master/3-features/Predictions-500/002.jpg?raw=1)\n",
        "![](https://github.com/zentralwerkstatt/MAT594SP/blob/master/3-features/Predictions-500/034.jpg?raw=1)  \n",
        "![](https://github.com/zentralwerkstatt/MAT594SP/blob/master/3-features/Predictions-500/122.jpg?raw=1)\n",
        "![](https://github.com/zentralwerkstatt/MAT594SP/blob/master/3-features/Predictions-500/310.jpg?raw=1)\n",
        "![](https://github.com/zentralwerkstatt/MAT594SP/blob/master/3-features/Predictions-500/416.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAAwOeQcxrA4",
        "colab_type": "text"
      },
      "source": [
        "### Random 100 filters of InceptionV1 layer Mixed_4c_Concatenated as per the hyperparameters defined above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcaoU-MyxrA5",
        "colab_type": "text"
      },
      "source": [
        "![](https://github.com/zentralwerkstatt/MAT594SP/blob/master/3-features/4C-100R/montage.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWUxIavXxrA6",
        "colab_type": "text"
      },
      "source": [
        "### Random 100 filters of InceptionV1 layer Mixed_5b_Concatenated as per the hyperparameters defined above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_OfELrsxrA7",
        "colab_type": "text"
      },
      "source": [
        "![](https://github.com/zentralwerkstatt/MAT594SP/blob/master/3-features/5B-100R/montage.jpg?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHlQcQQKxrA8",
        "colab_type": "text"
      },
      "source": [
        "## Bibliography\n",
        "\n",
        "- Dosovitskiy, Alexey, and Thomas Brox. \"Generating Images with Perceptual Similarity Metrics Based on Deep Networks.\" In Advances in Neural Information Processing Systems, 658‚Äì66, 2016. http://papers.nips.cc/paper/6157-generating- images-with-perceptual-similarity-metrics-based-on-deep-networks.\n",
        "- Nguyen, Anh, Alexey Dosovitskiy, Jason Yosinski, Thomas Brox, and Jeff Clune. \"Synthesizing the Preferred Inputs for Neurons in Neural Networks via Deep Generator Networks.\" In Advances in Neural Information Processing Systems,\n",
        "3387‚Äì95, 2016. http://papers.nips.cc/paper/6519-synthesizing-the-preferred- inputs-for-neurons-in-neural-networks-via-deep-generator-networks.\n",
        "- Nguyen, Anh, Jason Yosinski, Yoshua Bengio, Alexey Dosovitskiy, and Jeff Clune. \"Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space.\" arXiv Preprint, 2017. https://arxiv.org/abs/1612.00005.\n",
        "- Nguyen, Anh, Jason Yosinski, and Jeff Clune. \"Multifaceted Feature Visualization: Uncovering the Different Types of Features Learned by Each Neuron in Deep Neural Networks.\" arXiv Preprint arXiv:1602.03616, 2016. https://arxiv.org/abs/1602.03616.\n",
        "- Olah, Chris, Alexander Mordvintsev, and Ludwig Schubert. \"Feature Visualization.\" Distill, 2017. https://distill.pub/2017/feature-visualization.\n",
        "- Olah, Chris, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine Ye, and Alexander Mordvintsev. \"The Building Blocks of Interpretability.\" Distill, 2018. https://distill.pub/2018/building-blocks/\n",
        "- Simonyan, Karen, Andrea Vedaldi, and Andrew Zisserman. \"Deep inside convolutional networks: Visualising image classification models and saliency maps.\" arXiv preprint arXiv:1312.6034, 2013. https://arxiv.org/abs/1312.6034\n",
        "- Szegedy, Christian, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. \"Intriguing Properties of Neural Networks.\" arXiv Preprint arXiv:1312.6199, 2013. https://arxiv.org/abs/1312.6199"
      ]
    }
  ]
}